---
title:  "딥러닝 - 09. 워드 임베딩(Word Embedding)"
excerpt: ""

categories:
  - Deep Learning
tags:
  - [AI, 딥러닝, 파이썬, 자연어 처리, NLP]

toc: true

last_modified_at: 2025-01-02
thumbnail: ../images/2024-12-04-11-03-02.png
---

# 09. 워드 임베딩(Word Embedding)

## 09-01 워드 임베딩(Word Embedding)
:자연어 처리(NLP) 분야에서 단어를 컴퓨터가 이해 가능한 숫자 벡터, 즉 고차원 공간의 점으로 표현하는 기술     
이는 단순한 숫자 매핑을 넘어, 단어의 의미 및 문맥적 정보를 벡터에 담아 단어 간 관계를 수학적으로 계산 가능하게 함을 목표로 한다.
- 기존의 단어 표현 방식인 원-핫 인코딩은 단어 간 의미적 유사성을 나타내지 못하며, 벡터 차원이 단어 개수만큼 증가하는 희소 표현의 문제점을 가진다. 워드 임베딩은 이러한 문제점을 해소하고 단어 의미를 효과적으로 표현하기 위해 도입되었다.
- **장점**:
    - 단어 간 의미적 유사도 계산 가능: "개"와 "고양이"는 "자동차"와 "비행기"보다 의미적으로 가깝다는 사실을 벡터 간 거리 계산을 통해 도출할 수 있다.
    - 효율적인 계산: 고차원 희소 벡터 대신 저차원 밀집 벡터를 사용함으로써 계산 효율성을 제고한다.
    - 일반화 성능 향상: 모델이 학습하지 않은 단어에 대해서도 유사한 단어의 정보를 활용하여 성능 향상을 기대할 수 있다.

## 09-02 워드투벡터(Word2Vec)
: Word2Vec은 단어 주변 단어 정보를 활용하여 단어를 벡터로 임베딩하는 대표적인 방법.     
대량의 텍스트 데이터에서 단어 동시 출현 빈도를 학습하여 단어 벡터를 생성한다.
- **두 가지 학습 방식**:
    - CBOW(Continuous Bag of Words): 주변 단어 벡터를 평균내어 중심 단어를 예측한다. 예컨대, "The cat sat on the mat" 문장에서 "sat"을 예측하기 위해 "The", "cat", "on", "the", "mat"의 벡터를 활용한다.
    - Skip-Gram: 중심 단어 벡터를 이용하여 주변 단어들을 예측한다. 상기 예시에서 "sat" 벡터를 이용하여 "The", "cat", "on", "the", "mat"을 예측한다. Skip-gram이 CBOW 대비 우수한 성능을 보이는 경우가 많다.
- **학습 과정**: 대량의 텍스트 데이터를 슬라이딩 윈도우 방식으로 분석하여 중심 단어와 주변 단어 쌍을 생성하고, 이를 이용하여 신경망 모델을 학습한다.


## 09-04 네거티브 샘플링을 이용한 Word2Vec 구현(Skip-Gram with Negative Sampling)

- **문제점**: Skip-gram은 중심 단어 하나에 대해 주변 단어 전체를 대상으로 확률 계산을 수행해야 하므로 계산량이 과다하다.
- **해결책**: 네거티브 샘플링은 실제 주변 단어(Positive Sample) 외에 임의의 단어(Negative Sample)를 추가하여 이진 분류 문제로 변환한다. 이를 통해 계산량을 대폭 감소시킬 수 있다.
- 중심 단어와 실제 주변 단어 쌍에는 1의 레이블을, 중심 단어와 임의의 단어 쌍에는 0의 레이블을 부여하는 방식으로 로지스틱 회귀 모델을 학습한다.

## 09-05 글로브(GloVe)
: GloVe(Global Vectors for Word Representation)는 Word2Vec과 달리 전체 말뭉치에서의 단어 동시 발생 빈도 행렬을 이용하여 단어 벡터를 학습한다.
- **학습 방식**: 동시 발생 빈도 행렬을 분해하여 단어 벡터를 획득한다.
- **장점**: 전체 말뭉치의 통계 정보를 활용하므로 Word2Vec 대비 학습 속도가 빠르며, 작은 데이터셋에서도 양호한 성능을 나타낼 수 있다.

## 09-06 패스트텍스트(FastText)
: FastText는 Word2Vec의 확장된 형태로, 단어를 n-gram으로 분리하여 학습한다.     
예시: "apple"은 "ap", "pp", "pl", "le" 등의 n-gram으로 분리된다.
- **장점**:
    - OOV(Out-of-Vocabulary) 문제 해결: 학습 데이터에 존재하지 않는 단어라도 n-gram 조합으로 표현 가능하므로 OOV 문제에 강인하다.
    - 형태학적 정보 반영: 단어의 형태적인 특징(접두사, 접미사 등)을 반영할 수 있다. 특히 한국어와 같이 형태 변화가 많은 언어에서 유용하다.

## 09-07 자모 단위 한국어 FastText 학습하기

- 한국어는 형태소 분석이 중요하지만, 자모 단위로 FastText를 학습하면 형태소 분석 없이도 일정 수준의 성능을 확보할 수 있다.
- 자모 단위로 분리된 단어를 n-gram으로 처리하여 학습을 진행한다.

## 09-08 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)

- 대량의 데이터(예: Wikipedia, Google News)로 사전 훈련된 워드 임베딩 모델을 활용하면 시간 및 자원을 절약할 수 있다.
- 대표적인 사전 훈련 모델로는 Word2Vec, GloVe, FastText 등이 있으며, 공개된 모델을 다운로드하여 활용 가능하다.

## 09-09 엘모(Embeddings from Language Model, ELMo)

### 기존 워드 임베딩의 한계:
기존 워드 임베딩은 단어 하나에 하나의 벡터만 할당하는 정적인 임베딩이다. 즉, "배"가 "먹는 배"인지 "타는 배"인지에 따라 의미가 달라지지만, 동일한 벡터로 표현된다.
### ELMo의 특징
ELMo는 문맥 정보를 반영하는 동적인 임베딩이다. 양방향 LSTM 네트워크를 사용하여 단어의 전후 문맥을 모두 고려하여 벡터를 생성한다.
- 장점: 단어 의미가 문맥에 따라 변화하는 양상을 효과적으로 반영할 수 있다.

## 09-10 임베딩 벡터의 시각화(Embedding Visualization)

- 고차원 임베딩 벡터를 2차원 또는 3차원으로 차원 축소하여 시각적으로 표현하는 기술이다.
- t-SNE 등의 알고리즘을 활용하여 단어 간 관계를 시각적으로 확인 가능하다.

## 09-11 문서 벡터를 이용한 추천 시스템(Recommendation System)

- 문서들을 벡터로 표현하여 문서 간 유사도를 계산하고, 이를 활용하여 추천 시스템을 구축할 수 있다.
- 사용자-아이템 행렬 대신 문서 벡터를 사용하여 콘텐츠 기반 추천을 구현할 수 있다.

## 09-12 문서 임베딩 : 워드 임베딩의 평균(Average Word Embedding)

문서 내 단어들의 워드 임베딩을 평균하여 간단하게 문서 벡터를 생성할 수 있다.

## 09-13 Doc2Vec으로 공시 사업보고서 유사도 계산하기

Doc2Vec(Paragraph Vector): 문서를 벡터로 임베딩하는 알고리즘.       
Word2Vec과 유사한 방식으로 학습하지만, 문서 ID를 추가적인 입력으로 사용하여 문서 벡터를 학습한다.