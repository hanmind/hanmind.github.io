---
title:  "(TIL) 파이썬 - +True에서 `+`의 의미, 챗봇 - Text Loop 문제 "
excerpt: "`+` 연산자
- Python에서 + 연산자는 불리언 값(True 또는 False)을 **정수**로 변환한다.
    - True → 1
    - False → 0
결과적으로 이 코드는 동일하게 1 또는 0을 반환한다!"

categories:
  - TIL
tags:
  - [AI, 딥러닝, 파이썬, SQL, TIL, Django, 장고]

toc: true

last_modified_at: 2025-01-07
thumbnail: ../images/TIL.png
---
![](/images/../images/TIL.png)

# 알고리즘 공부
## [A로 B 만들기](https://school.programmers.co.kr/learn/courses/30/lessons/120886)
```py
# 오답: 역순만 고려하는 코드
def solution(before, after):
    before_rev = ''
    for i in range(len(before), 0, -1):
        before_rev += before[i-1] 
    return 1 if before_rev == after else 0
     
# 방법 1
def solution(before, after):
    # 두 문자열을 정렬하고 비교
    return 1 if sorted(before) == sorted(after) else 0

# 방법 2
def solution(before, after):
    return +(sorted(before) == sorted(after))
```         
### `+` 연산자
- Python에서 + 연산자는 불리언 값(True 또는 False)을 **정수**로 변환한다.
    - True → 1
    - False → 0
결과적으로 이 코드는 동일하게 1 또는 0을 반환한다!

# 캐릭터 챗봇

## Trainer 임포트 에러
[ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U'](https://velog.io/@ryan01/ImportError-Using-the-Trainer-with-PyTorch-requires-accelerate0.20.1-Please-run-pip-install-transformerstorch-or-pip-install-accelerate-U)

어제부터 끙끙 앓았는데, 단순 install -U 명령어로 해결이 되었다!

## Text Loop 문제
결과:       
```
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.

<input> Harry, are you ready for the challenge? <output>inputneither of you have much time for today.Plank.   Your attention please. Your headmaster. You must be going. It is time to go. I beg a few moments of your attention. Severus. <input><output><input name="Harry Potter"></output></input></output>" Educational Decrees. Personally I'm not sure I would have considered it necessary to ask you this question. But I do. Because I know that you are capable of thinking
```

결과에 `<input>`, `<output>`이 반복되는 현상이 나타났다. 이 이상한 반복 현상은 지금 내가 파인튜닝한 모델뿐만 아니라 전에 구현하던 코드에서도 본 적이 있었다. 뭔가 구조의 문제가 있지 않을까 생각이 들었고, 아래의 개념들을 찾았다.

### 1. attention_mask   
: Transformer 모델(예: GPT-2, BERT 등)이 입력 시퀀스에서 실제 의미 있는 토큰과 **패딩(padding)**을 구분할 수 있도록 도와주는 마스크

- 의미 있는 토큰: 입력 문장의 실제 단어 또는 심볼
- 패딩(padding): 입력 문장이 고정된 길이(max_length)보다 짧을 경우, 부족한 부분을 채우기 위해 추가된 토큰(주로 [PAD])

예시:       
문장이 고정된 길이보다 짧을 때, attention_mask는 의미 있는 토큰(1)과 패딩 토큰(0)을 구분한다.

```python
# 입력 토큰
input_ids = [101, 2023, 2003, 1037, 2742, 102, 0, 0]  # 8개의 토큰 (뒤의 두 개는 패딩)

# attention_mask
attention_mask = [1, 1, 1, 1, 1, 1, 0, 0]  # 의미 있는 토큰은 1, 패딩은 0
``` 
- `1`: 모델이 계산할 때 주목해야 할 토큰
- `0`: 모델이 무시해야 할 패딩 토큰

이렇게 하면 모델이 패딩된 부분을 무시하므로, 패딩이 모델의 출력 결과에 영향을 미치지 않도록 방지한다.

### 2. 패딩 토큰
: Transformer 모델은 **입력 길이가 일정해야** 학습과 추론이 가능하므로, 길이가 짧은 문장에 패딩 토큰을 추가하여 길이를 맞춘다.

**예시**             
길이를 8로 고정했을 경우:
- 문장 A: "Hello, world!"
    - 토큰화 후: [101, 7592, 1010, 2088, 999, 102] → 길이: 6
    - 패딩 추가: [101, 7592, 1010, 2088, 999, 102, 0, 0]
- 문장 B: "Hi!"
    - 토큰화 후: [101, 7632, 999, 102] → 길이: 4
    - 패딩 추가: [101, 7632, 999, 102, 0, 0, 0, 0]

**패딩 토큰 설정**
- [PAD]: 패딩을 위한 특수 토큰
- GPT-2에서는 기본적으로 패딩 토큰이 없으므로, 수동으로 설정해야 한다.

### 3. attention_mask와 패딩 토큰의 관계
패딩 토큰은 단순히 입력 길이를 맞추기 위해 추가한 것이므로, 학습이나 예측에서 패딩 부분은 무시해야 한다. -> 이를 위해 attention_mask가 필요!
1: 의미 있는 토큰(실제 단어)
0: 무시할 토큰(패딩 토큰 등)

### 4. 현재 코드에서 적용
에러 메시지:        
```
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
```

현재 코드에서는 attention_mask를 명시적으로 설정하지 않았기 때문에, 모델이 패딩 토큰을 의미 있는 데이터로 잘못 인식할 가능성이 있다.

이 위험은 아래와 같이 attention_mask를 추가하여 해결할 수 있다:  

```py
with torch.no_grad():
    outputs = model.generate(
        inputs['input_ids'],
        attention_mask=inputs['attention_mask'], # attention_mask 추가
        max_new_tokens=100,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.7,
        # top_k=50,  # 상위 50개의 단어만 고려
        # top_p=0.9,  # 확률 90%를 커버하는 단어만 선택
        pad_token_id=tokenizer.pad_token_id,  # 패딩 토큰 설정
    )
```

⁉ *Transformer의 generate 함수는, 패딩 토큰이 설정되지 않으면 기본적으로 종료 토큰(eos_token_id)을 패딩 토큰으로 간주한다. 실제로 내 코드에서도 알아서 종료 토큰 50256을 패딩 토큰으로 간주하는 것 같았다. 그렇지만 경고가 뜨는 것도 거슬리고, 보다 안정적으로 설정하기 위해 pad_token_id=tokenizer.pad_token_id도 함께 적어보았다.  
이 부분은 확실하게 공부하고 다시 정리하도록 하겠다.*

따라서 위 코드와 같이 outputs = model.generate()에 `attention_mask=`, `pad_token_id=`을 둘다 명시해주었다.

### Special Tockens

나는 챗봇을 만들기 위해 모델에 `<input> 내용 <output> 내용` 형식의 데이터셋을 학습시켰다. 그런데 그렇게 했더니, 모델이 답변에 `<input>`과 `<output>`이 엉망으로 들어간 텍스트를 내놓는 것 아닌가! 알고 보니, `<input>`과 `<output>`같은 특별한 토큰을 사용할 경우에는 이를 명시적으로 Special Tokens에 추가하는 것이 좋다고 한다. 이렇게 하면 토크나이저와 모델이 학습 및 추론 시 이러한 토큰들을 적절히 처리하여 토큰화/디토큰화 과정에서 혼란을 방지할 수 있다.       
```py
special_tokens = {'additional_special_tokens': ['<input>', '<output>'], 'pad_token': '[PAD]'}
tokenizer.add_special_tokens(special_tokens)
model.resize_token_embeddings(len(tokenizer))
```

### 부가설명 - `<input>`과 `<output>`을 Special Tokens에 추가해야 하는 이유
- `<input>`과 `<output>`은 기본적으로 GPT-2의 단어 사전에 포함되어 있지 않으므로, 별도의 처리 없이 사용하면 토크나이저가 이를 여러 개의 서브토큰으로 분리한다.
예를 들어, `<input>`이 [<, in, put, >]와 같이 여러 서브토큰으로 나뉘면, 학습과 추론이 일관성을 잃을 수 있다.
- 모델이 특정 역할(예: 입력과 출력 구분)로 사용되는 토큰을 명확히 이해하게 해준다.

# Django 공부
Project를 만든다 == 새로운 개발을 시작한다      
A라는 웹사이트를 만들려고 하면 Project 하나를 만들게 되는 것이다.

django는 MTV; Model, Template, View의 구조로 이루어져 있다.

# 오늘의 회고
오랜만에 알차게 집중해서 했다 ^^* 코드카타 시간도 팀원 분들과 타임어택 식으로 진행하니, 덕분에 짧은 시간에 밀도있게 코드를 짜게 되는 것 같다.

**챗봇 구현 To-do List**    
[] 이제 Albus Dumbledore 말고도 Harry Potter, Ron Weasley, Hermione Granger, Lucius Malfoy, Severus Snape 등 다른 캐릭터에 대해서도 진행       
[] 1 에폭에서 이미 loss가 감소하며 왔다갔다해서, batch나 에폭을 바꿔도 좋을듯. batch 크기는 어떤 기준으로 정하는가?     
[] 포터위키 등의 내용 크롤링해서 context/content로 넣기     
[v] Q-A 쌍으로 전처리       
[] 앞 대사도 같은 캐릭터이면, 그전 대사를 인풋으로 넣기? 또는 해당 데이터는 제외?(질문에 대한 대답이 아니므로)      
![](/images/../images/2025-01-07-16-29-31.png)      
[] 모델 epochs, batch 조정하기      
[] GPT-2 large 코드를 테스트할 때 봤던 것 같은 문장 반복은 데이터 더 넣고 파인튜닝하면 자연히 해결되는지도 확인하기  