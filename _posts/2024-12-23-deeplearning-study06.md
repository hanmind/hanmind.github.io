---
title:  "딥러닝 - 06"
excerpt: ""

categories:
  - Deep Learning
tags:
  - [AI, 딥러닝, 파이썬]

toc: true

last_modified_at: 2024-12-23
thumbnail: ../images/2024-12-04-11-03-02.png
---

# 머신 러닝

## 머신러닝 훑어보기
머신 러닝은 데이터가 주어지면, 기계가 스스로 **데이터로부터 규칙성을 찾는다**.
- 훈련(training)/학습(learning): 주어진 데이터로부터 규칙성을 찾는 과정

### 하이퍼파라미터 & 매개변수
- 하이퍼파라미터(초매개변수): 모델의 성능에 영향을 주는 사람이 값을 **지정**하는 변수.
  - 예시: 선형 회귀 中 경사 하강법에서의 학습률(learning rate), 딥러닝에서 뉴런의 수나 층(layer)의 수
- 파라미터(매개변수): 가중치와 편향. **학습을 하는 동안 값이 계속해서 변하는** 수.

### 튜닝(tuning)
훈련용 데이터로 **훈련**을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 **검증**하며 하이퍼파라미터를 튜닝(tuning)한다.

### 평가
이렇게 검증용 데이터의 정확도를 높여가며 모델 튜닝 과정을 끝냈다고 하자. 이제 모델을 최종 평가할 때는 모델이 한 번도 보지 못한 데이터인 테스트 데이터를 사용해야 한다.

### 분류와 회귀
- 이진 분류(Binary Classification): 주어진 입력에 대해서 두 개의 선택지 중 하나의 답을 선택
- 다중 클래스 분류(Multi-class Classification)
- 회귀(Regression): 몇 개의 정해진 선택지가 아닌, 연속적인 값의 범위 내에서 예측값이 나오는 문제
  - 예: 역과의 거리, 인구 밀도, 방의 개수 등을 입력하면 부동산 가격을 예측하는 머신 러닝 모델

### 지도 학습과 비지도 학습, 자기지도 학습
- 지도 학습(Supervised Learning): 레이블(Label)이라는 정답과 함께 학습하는 것.
  - 자연어 처리는 대부분 지도 학습에 속함. 자연어 처리의 많은 문제들은 레이블이 존재하는 경우가 많기 때문
- 비지도 학습(Unsupervised Learning): 데이터에 별도의 레이블이 없이 학습하는 것
  - 예: 토픽 모델링 알고리즘인 LSA나 LDA
- **자기지도 학습**(Self-Supervised Learning, SSL): 레이블이 없는 데이터가 주어지면, 모델이 학습을 위해서 스스로 데이터로부터 레이블을 만들어서 학습하는 것
  - 예: Word2Vec과 같은 워드 임베딩 알고리즘, BERT 등의 언어 모델

## 혼동 행렬(Confusion Matrix)
머신 러닝에서는 맞힌 문제수 / 전체 문제수 값을 정확도(Accuracy)라고 한다. 그런데 정확도는 이 결과에 대한 세부적인 내용을 알려주지는 않는다. 이를 위해서 사용하는 것이 혼동 행렬이다.
실제\예측 | 예측 참 | 예측 거짓
---|---|---
실제 참 | TP | FN
실제 거짓 | FP | TN
- True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)
- True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)
- False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)
- False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)

### 정밀도(Precision)
모델이 True라고 분류한 것 중에서 실제 True인 것의 비율

### 재현율(Recall)
실제 True인 것 중에서 모델이 True라고 예측한 것의 비율
Precision이나 Recall은 모두 실제 True인 정답을 모델이 True라고 예측한 경우. 즉, TP에 관심이 있다.
F1-Score는 나중 챕터에서 설명할 예정이다.

## 과적합
모델이 훈련 데이터에 대해서만 과하게 학습하여, 테스트 데이터나 실제 서비스에서는 성능이 좋지 않은 현상
~[](https://wikidocs.net/images/page/32012/%EC%8A%A4%ED%8C%B8_%EB%A9%94%EC%9D%BC_%EC%98%A4%EC%B0%A8.png)
과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 커진다.

## 과소적합(Underfitting)
테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태
- 훈련 횟수(에포크)가 지나치게 적으면 발생
- 훈련 데이터에 대해서도 정확도가 낮다.
+a. 과적합과 과소 적합이라고 부르는 이유:
머신러닝에서는 학습 또는 훈련이라고 하는 과정을 **적합(fitting)**이라고도 부른다.

## 옵티마이저(Optimizer)
: 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 w(가중치; weight)와 b(편향; bias)을 찾기 위한 작업

### 경사하강법(Gradient Descent)
대표적인 옵티마이저의 일종.
가설 H(x): y = wx
비용함수: cost(w)
![](https://wikidocs.net/images/page/21670/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%803.JPG)
그림에서 cost가 최소화가 되는 지점은 접선의 기울기, 즉 미분값이 0이 되는 지점이다.
-> 경사 하강법의 아이디어는 비용 함수(Cost function)를 미분하여 현재 w
에서의 접선의 기울기를 구하고, 접선의 **기울기가 낮은 방향으로**
의 w값을 변경한 뒤 다시 미분하는 과정을 반복하면서 접선의 기울기가 0인 곳을 찾아나가는 것!

## 로지스틱 회귀
이진 분류 문제를 해결하는 알고리즘.
- 0.5를 판단 기준으로 했을 때, 최종 예측값이 0.5보다 작으면 0으로 예측했다고 판단하고, 0.5보다 크면 1로 예측했다고 판단
- 출력이 0과 1사이의 값을 가지면서 S자 형태로 그려지는 함수가 필요
  - 예: 시그모이드 함수

![시그모이드 함수](https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%EA%B7%B8%EB%9E%98%ED%94%84.png)
![](https://wikidocs.net/images/page/22881/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C%ED%95%A8%EC%88%98%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%9D%98%EB%B3%80%ED%99%94.png)

형 회귀에서 직선을 표현할 때, 가중치 w
는 직선의 기울기를 의미했지만 여기서는 그래프의 경사도를 결정.
w의 값이 커지면 경사가 커지고
w의 값이 작아지면 경사가 작아진다.
+a. 로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.

## 다중 입력에 대한 실습
딥러닝에선 대부분 독립 변수가 2개 이상이다.

![](/images/../images/다중입력%20표.png)

## 인공 신경망 다이어그램

다중 로지스틱 회귀를 인공 신경망의 형태로 표현하면 다음과 같다. 로지스틱 회귀는 일종의 인공 신경망 구조로 해석할 수 있다.

![](https://wikidocs.net/images/page/35821/perceptron.JPG)

## 벡터와 행렬 연산
선형 회귀와 로지스틱 회귀와 달리  소프트맥스 회귀에서는 종속 변수
의 종류도 3개 이상이 되면서 더욱 복잡하다.
- 스칼라: 0차원 텐서
- 벡터: 숫자를 배열한 것. 1차원 텐서
  - `d = np.array([1, 2, 3, 4])`는 4차원 벡터이자 1차원 텐서
- 행렬(matrix): 행과 열이 존재하는 벡터의 배열. 2차원 텐서
- 다차원 배열: 3차원 텐서
자연어 처리에서는 3D 텐서를 자주 보게 된다. 3D 텐서는 시퀀스 데이터(sequence data)를 표현할 때 자주 사용되기 때문.
시퀀스는 주로 문장이나 문서, 뉴스 기사 등의 텍스트가 될 수 있다.