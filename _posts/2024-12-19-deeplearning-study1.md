---
title:  "딥러닝 - 01. 자연어 처리"
excerpt: "딥러닝 관련 주요 라이브러리
- **텐서플로우(Tensorflow)**: 구글의 머신 러닝 오픈소스 라이브러리
    - 머신 러닝을 직관적이고 손쉽게 설계할 수 있다.
- **케라스(Keras)**: 백엔드로 텐서플로우를 사용하면서, 좀 더 쉽게 딥러닝을 사용할 수 있게 해주는 라이브러리"

categories:
  - Deep Learning
tags:
  - [AI, 딥러닝, 파이썬, 자연어 처리, NLP]

toc: true

last_modified_at: 2024-12-19
thumbnail: ../images/TIL.png
---

# 01. 자연어 처리
## 딥러닝 관련 주요 라이브러리
- **텐서플로우(Tensorflow)**: 구글의 머신 러닝 오픈소스 라이브러리
    - 머신 러닝을 직관적이고 손쉽게 설계할 수 있다.
- **케라스(Keras)**: 백엔드로 텐서플로우를 사용하면서, 좀 더 쉽게 딥러닝을 사용할 수 있게 해주는 라이브러리
    - 코드를 훨씬 간단하게 작성할 수 있다.
    - 단, 모듈화의 한계로 복잡한 프로젝트에 구현 범위가 다소 좁은 편
    - 현재는 텐서플로우에서 바로 케라스 API를 사용할 수가 있기 때문에, 앞으로는 순수 keras보다 tf.keras를 사용할 것을 권장한다.
- **젠심(Gensim)**: 머신러닝으로 토픽 모델링과 자연어 처리 등을 수행할 수 있게 해주는 오픈소스 라이브러리
- **사이킷런(Scikit-learn)**: 파이썬 머신러닝 라이브러리
    - 나이브 베이즈 분류, SVM 등 다양한 머신 러닝 모듈 import 가능
    - 아이리스 데이터, 당뇨병 데이터 등 자체 데이터 제공

## 자연어 처리를 위한 NLTK와 KoNLPy
- NLTK: 자연어 처리를 위한 파이썬 패키지 (아나콘다 보유 O)
- KoNLPy(코엔엘파이): **한국어** 자연어 처리를 위한 형태소 분석기 패키지 (아나콘다 보유 X)

## Pandas
- example.csv   
![](https://wikidocs.net/images/page/32829/sample.PNG)   
```py
df = pd.read_csv('example.csv')
print(df)
```

결과:
```
   student id      name  score
0        1000     Steve  90.72
1        1001     James  78.09
2        1002    Doyeon  98.43
3        1003      Jane  64.19
4        1004  Pilwoong  81.30
5        1005      Tony  99.14
```
Pandas 데이터프레임에 csv를 넣어주면 인덱스가 0부터 순서대로 자동 부여된다.   
```py
print(df.index) # 인덱스 출력
```

결과:
```
RangeIndex(start=0, stop=6, step=1)
```

## +a. 넘파이 배열의 차원 및 크기 쉽게 파악하기!
가장 바깥쪽 괄호부터 내부의 괄호를 세면 차원을 쉽게 구할 수 있다. 또한, 배열의 크기는 각 차원의 덩어리 개수로 파악할 수 있다. 예를 들어:   
### 1차원 배열
```py
# 1차원 배열
vec = np.array([1, 2, 3, 4, 5])
print(vec)
print('vec의 축의 개수 :',vec.ndim) # 축의 개수 출력
print('vec의 크기(shape) :',vec.shape) # 크기 출력
```

결과:
```
[1 2 3 4 5]
vec의 축의 개수 : 1
vec의 크기(shape) : (5,)
```

### 2차원 배열
```py
# 2차원 배열
mat = np.array([[10, 20, 30], [60, 70, 80]]) 
print(mat)
print('mat의 축의 개수 :',mat.ndim) # 축의 개수 출력
print('mat의 크기(shape) :',mat.shape) # 크기 출력
```

결과:
```
[[10 20 30]
 [60 70 80]]
mat의 축의 개수 : 2
mat의 크기(shape) : (2, 3)
```

### 4차원 배열
```py
# 4차원 배열
mat_4d = np.array([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]],
                   [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]])

print(mat_4d)  # 배열 출력
print('mat_4d의 축의 개수 :', mat_4d.ndim)  # 축의 개수 출력
print('mat_4d의 크기(shape) :', mat_4d.shape)  # 크기 출력
```
결과:
```
[[[[ 1  2]
   [ 3  4]]

  [[ 5  6]
   [ 7  8]]]


 [[[ 9 10]
   [11 12]]

  [[13 14]
   [15 16]]]]
mat_4d의 축의 개수 : 4
mat_4d의 크기(shape) : (2, 2, 2, 2)
```

## ndarray 초기화
- np.zeros()는 배열의 모든 원소에 0을 삽입
    - 예시: `np.zeros((2,3))`
- np.ones()는 배열의 모든 원소에 1을 삽입
    - 예시: `np.ones((2,3))`
- np.full()은 배열에 사용자가 지정한 값을 삽입
    - 예시: `np.full((2,2), 7)`
- np.eye()는 대각선으로는 1이고 나머지는 0인 2차원 배열을 생성
    - 예시: `np.eye(3)` → 3x3 단위행렬 생성
    - 예시: `np.eye(N=3, M=5, k=1, dtype=int8)` → 3x5 행렬, identity matrix의 시작은 1열부터 시작   
        ```
        [[0, 1, 0, 0, 0], 
         [0, 0, 1, 0, 0], 
         [0, 0, 0, 1, 0]]
        ```

## 맷플롯립(Matplotlib)
맷플롯립(Matplotlib): 데이터를 차트(chart)나 플롯(plot)으로 시각화하는 패키지
- 데이터 분석 이전에 데이터 이해를 위한 시각화 / 데이터 분석 후 결과 시각화에 사용
- `plot()`: **라인 플롯**을 그리는 기능 수행

```py
plt.title('students') # 제목 지정
plt.plot([1,2,3,4],[2,4,8,6]) # 라인 플롯
plt.plot([1.5,2.5,3.5,4.5],[3,5,8,10]) # 라인 새로 추가
plt.xlabel('hours') # x축 레이블
plt.ylabel('score') # y축 레이블
plt.legend(['A student', 'B student']) # 범례 삽입
plt.show() # 시각화
```
![](https://wikidocs.net/images/page/32829/matplotlib3.PNG)

## 머신 러닝 워크플로우
![](https://wikidocs.net/images/page/31947/%EB%A8%B8%EC%8B%A0_%EB%9F%AC%EB%8B%9D_%EC%9B%8C%ED%81%AC%ED%94%8C%EB%A1%9C%EC%9A%B0.PNG)

### 1) 수집(Acquisition)
### 2) 점검 및 탐색(Inspection and exploration)
### 3) 전처리 및 정제(Preprocessing and Cleaning)
### 4) 모델링 및 훈련(Modeling and Training)
### 5) 평가(Evaluation)
### 6) 배포(Deployment)
### 1. 수집(Acquisition)
- 머신 러닝에 필요한 데이터를 수집하는 단계
- 자연어 처리의 경우, 데이터를 **말뭉치 또는 코퍼스(corpus)**라고 부른다.
  - 코퍼스: 조사나 연구 목적으로 특정 도메인에서 수집된 텍스트 집합
  - 텍스트 데이터의 형식은 `txt`, `csv`, `xml` 등 다양하며, 출처 역시 음성 데이터, 웹 수집기, 영화 리뷰 등으로 다양하다.

---

### 2. 점검 및 탐색(Inspection and Exploration)
- 데이터를 점검하고 탐색하는 단계
- 데이터의 구조, 노이즈 데이터, 정제 필요성 등을 파악
- **탐색적 데이터 분석**(**EDA**, Exploratory Data Analysis)라고도 한다.
  - 독립 변수, 종속 변수, 변수 유형, 데이터 타입 등을 점검
  - 데이터의 특징과 구조적 관계를 알아내는 과정
- 시각화 및 간단한 통계 테스트를 진행하기도 함

---

### 3. 전처리 및 정제(Preprocessing and Cleaning)
- 가장 까다로운 작업 중 하나
- 자연어 처리의 경우, 전처리 작업은 다음을 포함할 수 있다:
  - 토큰화, 정제, 정규화, 불용어 제거 등
- 전처리 과정에서는 파이썬과 같은 툴과 라이브러리를 활용한다.
- 매우 복잡한 전처리의 경우, 전처리 과정에 머신 러닝이 사용되기도 한다.

---

### 4. 모델링 및 훈련(Modeling and Training)
- (1) 먼저 적절한 머신 러닝 알고리즘을 선택하여 모델링을 수행한다.
- (2) 전처리된 데이터를 기반으로 기계 학습을 진행한다.
- 이때 훈련 과정에서는 데이터의 일부를 테스트용으로 남겨두고, 나머지 데이터를 훈련에 사용한다.
  - **과적합**(overfitting)을 방지하기 위해 훈련용, 검증용, 테스트용 데이터로 나누는 것을 권장
- 검증용과 테스트용 데이터의 차이:
  - **검증용 데이터**: 모델 성능을 개선하기 위한 데이터. 학습 단계에서 사용된다.
  - **테스트용 데이터**: 모델의 최종 성능을 평가하기 위한 데이터. 성능 개선에 사용되지 않는다!   

![](https://wikidocs.net/images/page/31947/%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)
- 훈련용 데이터 = 학습지
- 검증용 데이터 = 모의고사
- 테스트용 데이터 = 수능 시험
---

### 5. 평가(Evaluation)
- 훈련이 완료된 후 모델의 성능을 평가하는 단계
- 기계가 예측한 데이터가 테스트 데이터의 실제 정답과 얼마나 가까운지를 측정한다.
- **성능 지표**를 통해 모델의 효과를 수치화한다.

---

### 6. 배포(Deployment)
- 평가 결과, 모델이 성공적으로 학습된 것으로 판단되면 배포하는 단계이다.
- 배포 후, 피드백에 따라 **모델을 업데이트**해야 하는 상황이 발생할 수 있으며, 이 경우 **수집 단계**로 되돌아갈 수 있다.