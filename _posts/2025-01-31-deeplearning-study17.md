---
title:  "(TIL) 자연어 처리 - 17. BERT"
excerpt: " "

categories:
  - Deep Learning
tags:
  - [AI, 딥러닝, 파이썬, 자연어 처리, NLP]

toc: true

last_modified_at: 2025-01-31
thumbnail: ../images/TIL.png
---
![](/images/../images/TIL.png)

# 17. BERT
트랜스포머(transformer)의 등장 이후 다양한 자연어 처리 태스크에서 사용되었던 RNN 계열의 신경망인 LSTM, GRU는 트랜스포머로 대체되어가는 추세입니다. 이에 따라 다양한 트랜스포머 계열의 BERT, GPT, T5 등 다양한 사전 훈련된 언어 모델들이 계속해서 등장하고 있다.

## 17-01 NLP에서의 사전 훈련(Pre-training)

워드 임베딩에서부터 ELMo, 그리고 트랜스포머에 이르기까지 자연어 처리의 역사를 정리해보자.

Word2Vec, FastText, GloVe와 같은 워드 임베딩 방법론들은 모두 하나의 단어가 하나의 벡터값으로 맵핑되므로, 문맥을 고려하지 못 하여 다의어나 동음이의어를 구분하지 못하는 문제점이 있다.       
예시: 사과 = forgive, apple

=> '사전 훈련된 언어 모델'인 ELMo나 BERT 등이 이러한 문제의 해결책이 된다고 함!

### 사전 훈련된 언어 모델

![](https://wikidocs.net/images/page/108730/image4.PNG)

우측에 있는 양방향 언어 모델은 지금까지 본 적 없던 형태의 언어 모델이다. 
양방향 LSTM을 이용해서 우측과 같은 언어 모델을 만들었다고 해보자. 초록색 LSTM 셀은 순방향 언어 모델로 <sos>를 입력받아 I를 예측하고, 그 후에 am을 예측한다. 그런데 am을 예측할 때, 출력층은 주황색 LSTM 셀인 역방향 언어 모델의 정보도 함께 받고있다.

=> am을 예측하는 시점에서 역방향 언어 모델이 이미 관측한 단어는 a, am, I로, 이미 예측해야하는 단어를 역방향 언어 모델을 통해 미리 관측한 셈이다.

하지만 언어의 문맥은 양방향으로 파악이 이루어진다. 따라서 대안으로 ELMo에서는 순방향과 역방향이라는 두 개의 단방향 언어 모델을 따로 준비하여 학습하는 방법을 사용했던 것이다.

=> 이와 같은 상황에서, **양방향 구조**를 도입하기 위해 탄생한 새로운 구조의 언어 모델이 **마스크드 언어 모델**이다.

### 마스크드 언어 모델
마스크드 언어 모델은 입력 텍스트의 단어 집합의 15%의 단어를 랜덤으로 마스킹(Masking)한다. 문장 중간에 구멍을 뚫어놓고, 구멍에 들어갈 단어들을 예측하게 하는 식!

## 17-02. 버트(Bidirectional Encoder Representations from Transformers, BERT)
![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC2.PNG)        
[CLS]라는 벡터는 BERT의 초기 입력으로 사용되었을 입력 임베딩 당시에는 단순히 임베딩 층(embedding layer)를 지난 임베딩 벡터였지만, BERT를 지나고 나서는 [CLS], I, love, you라는 모든 단어 벡터들을 모두 참고한 후에 문맥 정보를 가진 벡터가 됨.

이는 [CLS]라는 단어 벡터 뿐만 아니라 [CLS], I, love, you 모두 마찬가지. 출력 임베딩 단계의 각 단어에서 BERT의 입력이었던 모든 단어들인 [CLS], I, love, you를 참고하고 있다. (우측 그림 참고)

- BERT는 단어보다 더 작은 단위로 쪼개는 서브워드 토크나이저를 사용한다.
- 서브워드 토크나이저는 기본적으로 자주 등장하는 단어는 그대로 단어 집합에 추가하지만, 자주 등장하지 않는 단어의 경우에는 더 작은 단위인 서브워드로 분리되어 서브워드들이 단어 집합에 추가된다는 아이디어를 가진다.

예를 들어 embeddings이라는 단어가 입력으로 들어왔을 때, BERT의 단어 집합에 해당 단어가 존재하지 않았다면 해당 단어를 더 쪼개려고 시도한다. 만약, BERT의 단어 집합에 em, ##bed, ##ding, #s라는 서브 워드들이 존재한다면, embeddings는 em, ##bed, ##ding, #s로 분리된다. (##은 단어의 중간부터 등장하는 서브워드라는 것을 알려주는 기호)

### 포지션 임베딩
트랜스포머에서는 포지셔널 인코딩(Positional Encoding)이라는 방법을 통해서 단어의 위치 정보를 표현했다.      
❗ 포지셔널 인코딩: 사인 함수와 코사인 함수를 사용하여 위치에 따라 다른 값을 가지는 행렬을 만들어 이를 단어 벡터들과 더하는 방법       

BERT에서는 이와 유사하지만, 위치 정보를 사인 함수와 코사인 함수로 만드는 것이 아닌 **학습**을 통해서 얻는 포지션 임베딩(Position Embedding)이라는 방법을 사용한다.

![](https://wikidocs.net/images/page/115055/%EA%B7%B8%EB%A6%BC5.PNG)

- WordPiece Embedding: 우리가 이미 알고 있는 단어 임베딩으로 실질적인 입력
- Position Embedding: 위치 정보를 위한 임베딩 층

=> 입력에 포지션 임베딩을 통해서 위치 정보를 더해준다.      
- 첫번째 단어의 임베딩 벡터 + 0번 포지션 임베딩 벡터
- 두번째 단어의 임베딩 벡터 + 1번 포지션 임베딩 벡터
- 세번째 단어의 임베딩 벡터 + 2번 포지션 임베딩 벡터
- 네번째 단어의 임베딩 벡터 + 3번 포지션 임베딩 벡터