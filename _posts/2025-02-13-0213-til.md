---
title:  "(TIL) LLM 특강 4"
excerpt: "MLP(다층 퍼셉트론) 퍼셉트론은 선형 모델이어서 분류를 하지 못하는 문제가 있음. 예: XOR 문제"

categories:
  - Deep Learning
tags:
  - [AI, 딥러닝, 파이썬, 자연어 처리, NLP]

toc: true

last_modified_at: 2025-02-13
thumbnail: ../images/TIL.png
---
![](/images/../images/TIL.png)

# 알고리즘 공부
## [가장 가까운 같은 글자](https://school.programmers.co.kr/learn/courses/30/lessons/142086)

```py
# 방법 1: 시간복잡도 높음, 비추천
def solution(s):
    answer = [-1] * len(s)
    for i, char in enumerate(s): 
        for j in range(i - 1, -1, -1): # i-1 ~ 0번째 문자들 순회
            if s[j] == char:
                answer[i] = i-j
                break

# 방법 2: 딕셔너리 활용
def solution(s):
    last_seen = {}  # 문자의 마지막 위치를 저장할 딕셔너리
    answer  = []  # 결과를 저장할 리스트
    for i, char in enumerate(s):  # 문자열을 한 글자씩 탐색
        if char in last_seen:
            answer.append(i-last_seen[char])  # 현재 위치와 마지막 위치의 차이를 계산
        else:
            answer.append(-1)  # 처음 나온 문자라면 -1 추가
        last_seen[char] = i  # 현재 문자의 위치를 업데이트
    return answer
```

- 딕셔너리를 사용하면 편하다!
- 각 문자가 마지막으로 등장한 위치(인덱스) 를 기록할 딕셔너리를 만든다.
    - 예시: s = "banana"를 순회하면서 a가 나오면, "a": 1 같은 식으로 위치를 저장
- 새로운 문자가 나오면 -1을, 이미 나온 문자라면 '현재 위치 - 마지막 위치'를 계산해서 answer 리스트에 저장

# 임베딩과 RAG의 관계
## 임베딩
:텍스트를 숫자(**벡터**)로 변환하는 기술
중요한 점은 단순한 변환이 아니라 **의미를 보존**하면서 변환된다는 것이다! 그래서 비슷한 의미의 텍스트는 **비슷한 벡터 값**을 가지게 된다.

### 🔍 임베딩이 RAG에서 어떻게 활용될까?
1. 문서를 벡터화해서 벡터 DB에 저장
2. 사용자가 질문을 입력하면 질문도 벡터화
3. 유사도를 비교해 관련 문서(핵심 요약 노트)를 찾음
4. 찾은 문서를 질문과 함께 LLM에 제공
5. LLM이 관련 정보를 기반으로 답변 생성

### 🎯 임베딩의 품질 = RAG의 품질
결국 좋은 임베딩이 만들어져야(임베딩이 정확하게 의미를 보존해야) 정확한 핵심 요약 노트가 생성되고, 이것이 곧 RAG의 성능을 좌우한다. 즉, 임베딩 모델은 LLM이 잘 답변할 수 있도록 돕는 존재라고 볼 수 있다.

다만 현재 공부하고 있는 부트캠프에서는 임베딩 모델의 품질을 고민하기보다는 API를 사용하여 실제 서비스를 구현하는 것에 초점을 맞추고 있다. 현존하는 임베딩 모델을 잘 활용해 RAG 기능을 챗봇 서비스에 잘 적용해보자.
